# Deep Learning for Audio Signal Processing

## 概要
- オーディオ信号処理のためのDeep Learning技術のレビュー

## 導入
- 深層学習のルネッサンス
  - 1957年のパーセプトロンアルゴリズム[1]
  - 1986年のバックプロパゲーションアルゴリズム[2]
  - 2012年の音声認識[3]と画像分類[4]における深層学習の成功
  - ディープフィードフォワードニューラルネットワーク[3]、[5]
  - CNN[6]
  - LSTM[7]
- 最初に画像処理[4]で注目を集めたが、音声処理、音楽、環境音処理、さらにゲノミクス、量子化学、創薬、自然言語処理、レコメンデーションなど多くの分野で使われるようになった。
- ガウス混合モデル、隠れマルコフモデル、非負行列因数分解などの音声信号処理の古典的手法は、データが十分にある場合にはしばしばDeepに劣る。
- 画像と音声は本質的にデータの質が異なる。ある瞬間を切り取った二次元データと、時間で刻々と変化する一次元データでは適切な処理方法が異なる。

## 方法

### オーディオ分析・合成問題のカテゴライズ
- オーディオ時系列を入力とするのは共通だが、出力（どんなタスクを解かせたいか）によって分類が可能。
  - 出力方式での分類
    - 一つのオーディオに対しシングルラベル
    - オーディオのtime-stepごとにシングルラベル
    - 一つのオーディオに対し可変長出力
  - 出力形態での分類
    - シングルクラス
    - マルチクラス
    - 数値
- シークエンス分類は一つのオーディオに一つのラベルを付与するタスクの総称である。
  - 音声の言語を特定するタスク
  - 話者認識
- マルチラベルシークエンス分類は複数のラベルを付与するタスクの総称である。対応可能なクラスの部分集合を割り当てる。
- シークエンス回帰は一つのオーディオからある一つの値（もしくは固定長ベクトル）を当てるタスクの総称である。
- シークエンスラベリングを、オーディオのtime-stepごとに分類を行うタスクの総称とする。
  - コードアノテーション
  - 発声検知
  - イベント検知（どのtime-stepで発生したかを当てる）
    - 話者変更検知
    - onset検知
- 時間ステップごとに回帰を行う
  - ピッチ推定
  - 音源定位
  - 音源分離
- 可変長出力のタスクは特に名前とかない。
  - 音声認識（テキスト変換）
  - 自動採譜
  - 機械翻訳
- 入力がオーディオ信号でないタスクもある。
  - オーディオ信号でない別の条件の信号列からオーディオを合成するタスク（回帰？）

### 入力表現
- 特徴量を人手で作成するのは終わったよ！
  - Mohamedら[10]の研究で、DNNの下層は特徴量抽出、上層はクラス判別してると言う認識になった。
  - MFCCはDNNには不要。
  - メルフィルタバンクもあるけど、constant-Qスペクトラムは適切なフィルタバンクを作ってくれる？
  - 周波数表現は自然画像とは異なり、基本周波数の倍音にピークがでやすい特徴があるので考慮する必要がある。
  - スペクトラム計算時のwindowサイズは、高周波成分の方が小さくなるけど歪むよね。
- こう言うのいちいち考えるの面倒だし、フィルタをdata-drivenで学習しちゃえばいいよね。[24]とかmel-spectrumを意識して作ったフィルタ提案してる。

### モデル
- CNN
  - CNNが一度に見られる入力サイズ：receptive field=受容野
  - 広範囲を見るフィルタとして穴あき昆布（dilatedもしくはatrous）もいいよね！
  - **与えられたタスクに最適なCNNの構造を決定する理論は現時点で存在しない**
  - より少ないデータでより少ないパラメータを用いること [31]，後続の畳み込み層の特徴マップのサイズを小さくしてチャネル数を増やすこと，必要な時間的コンテキストのサイズを考慮すること，タスクに関連した設計（例えば解析や合成/変換）にすること。
- RNN
  - LSTMは学習の勾配の消失or爆発を軽減する
  - セルの積み重ね[34]とスパースリカレントネットワーク[35]は、オーディオ合成に有用
  - TF-LSTM（時間-周波数LSTM）と言うのがあり特定のタスクで威力を発揮するけど、並列処理できないので遅い。
  - CRNN(Conv-RNN)もいいよね
- seq2seq
  - 「オーディオ→可変長出力」タスクは複雑なことが多いので通常サブタスクを独立に訓練させる[40][41]。
  - end2endは難しい前処理や後処理がいらないから楽ちんだよねって注目を集めた。
    - CTC
    - Attention
- GAN
  - 低次元のランダムな潜在ベクトルから与えられたデータセットの現実的なサンプルを生成するために学習する教師なし生成モデル
  - Lossの設計について、時間方向のMSEをとるのはまずい。位相揺れとかあるし。DTWとかEMD(WGAN)とかの方がいいかもね。

### データ
- Deepには大量のデータが必要。
  - 画像：ImageNet
  - 英語の音声認識：[71]
  - 音楽シーケンスの分類や音楽の類似性について:Milion Song Dataset
  - 環境音：Audio Set ? 
- ImageNetで事前学習させるみたいに、機械翻訳分野でも、多くの学習データが存在する言語で事前学習を行う。

### 評価
- MOS(mean opinion score)アンケート調査のことかな？

## 応用

### 解析
- 音声認識
  - 音声認識の歴史は半世紀以上[87]
    - 最初はGaussioan MixtureやHMMが主流（美しい理論？）
    - ニューラルネットとのハイブリッド[88]-[90]
    - DNNによる劇的な単語エラー率の減少[3]
    - LSTMやGRUはDNNより良い性能[92]
    - CLDNN(Convolutional LSTM Deep Neural Net)はLSTMより良い性能[93]
      - まず信号の周波数分散を減らすために最大プーリング層を持つ2つの畳み込み層で処理
      - 次に、時間的相関をモデル化するため、LSTM層のための低次元の特徴空間に投影
      - 最後に、いくつかのフィードフォワード層と出力ソフトマックス層を通過
    - seq2seqに関心が寄せられる
      - LASモデルはエンコーダに音響モデル、デコーダに言語モデルを用いたモデル[94]で、とても認識性能がよろしい
    - データの少ない言語については転移学習が主流[75]
- 音楽認識
  - [99]に音楽認識のタスクとその解法のlistあり
  - 音楽データ解析の歴史
    - NNを最初に音楽データに適用したタスクが**オンセット検知**[84]
      - その後もオンセット検知タスクは改良が重ねられた
      - [16]ではlog-melをCNNで処理しよう、と言うタスク(でも入力スペクトログラムになってた...
    - ビートとダウンビートの推定に繋がる？
  - コード認識はマルチクラスシーケンスラベル付け問題
  - シークエンス分類
    - [109]でlog-melスペクトグラムに対する時間Convの方が生音声にConvをかけるよりも精度が高くなったことを示した。
    - CVでよく使われるaverage-poolingと比較して、max-poolingは、局所的な声の検出がグローバルな予測に昇格することを確実にするために選択された。
  - 音楽認識について、研究面では、どのような入力表現を使用するか（ログメルスペクトログラム、定数Q、生の音声）、どのようなアーキテクチャを使用するか（CNNまたはRNN、またはその両方、2Dまたは1Dの畳み込み、小さな正方形または大きな長方形のフィルタ）については、タスク内でもタスク間でもコンセンサスが得られておらず、さらなる研究のための多くの未解決の問題が残されています。
- 環境音解析
  - 音響シーン分類 
    - 環境音に対してそのシーン（レストラン、車内、）を割り当てる分類問題
    - [113]では、RNNの固定ベクトルの一つ一つの次元に環境音内のイベント（犬が泣いた、チャイムがなった、）の発生を割り当てる。
  - 環境音分析はあまり発展してないので、データセットもあんまりない。なのでオーグメンテーションとかも発達している。
- マルチチャンネルオーディオ
  - 音源が複数で別々の位置でなっている音
  - 音源分離、音源定位、ダイアリゼーションシステム（誰がどこで喋っているかを当てるタスク）
  - 音源分離で時間-周波数分解する理由
    1) 自然音源の構造は時間周波数領域でより顕著であり、時間領域の信号よりも容易にモデリングできること
    2) 周波数領域では瞬時の混合として近似できる音源からマイクロホンへの音響伝達関数を含む畳み込み混合で処理が単純化されること
    3) 自然音源は時間周波数領域では疎であるため、時間周波数領域での分離が容易であること、
    4) コンスタント-Qやmel-logでは情報が欠落しすぎてしまう
  - [123]では音源分離に特化した損失関数の説明？（MSEではダメらしい）
  - 最近の音源分離は、教師付きディープラーニングを使用して、各時間周波数点の埋め込みベクトルを推定し、その後に教師なしでクラスタ化する。
- speech enhancement
  - ノイズ除去？
  - GANを使うと耳障りはよくなるけど、他タスクの精度は上がらない。
- 生成モデル
  - Briotら[145]は、ディープラーニングを用いた音楽生成についてより詳細に説明している。
## 議論
- 従来はMFCCが一般的だったが、DNNではlog-melが主流。生の波形よりもいいことが多い。

## Point
- Convの受容野は大きい方がいいけど、パラメータは少ない方がいい


## 気になったこと
- [109]みたいに、逆に生データへのConvよりもlog-melとかMFCCとかの方が精度が良い場合を探したいよね。
- 