# Cascade R-CNN: High Quality Object Detection and Instance Segmentation

## Abstruct
物体検知の一種。<br>
IoUの閾値って検出精度に取ってトレードオフ(閾値が高いと検出精度下がるし、低いとノイズを拾いやすくなる)。<br>
IoUを段階的に変化させながら学習させるモデルを考案したよ！
Mask-RCNNより良くなったよ！

## Introduction
物体検知のタスクは二つ。
- 背景とオブジェクトを分離し、ラベルを割り当てるタスク
- それぞれのオブジェクトに適切なBounding Boxを割り振るタスク

先行研究の二段階R-CNNはこの二つのタスクをマルチタスク学習で解決している。
BBの推定について、「惜しいんだけどちょっと正解とずれている」推定をすることがよくあった。先行研究のIoU0.5はかなり緩い閾値。特に「人」検知ではFalse Positiveが多い。(間違って「人がいるぞ！」と言いがち。)

<!-- 「検出結果の品質」をground truthとのIoUと定義し、「検出器の品質」をIoUの閾値と定義する。 -->

ともかく、学習時に指定したIoUをテスト時にも使った方がいいんだけど、検出精度と品質のトレードオフなんだよね、と言うこと。
(訓練時のIoUあげると、学習に使えるデータが減って、過学習してしまう!?)

(提案手法のCascade-RCNNは段階的にIoUを上げていき、一つ前のIoUで検出された物体を使ってclassifierを学習するのか？)

ともかく<href>https://arxiv.org/abs/1712.00726</href>の方は初稿みたいなもんらしい。


Cascade RCNNの解説
→ <href>https://www.slideshare.net/grafi_tt/20180427-arxivtimes-cascade-rcnn-delving-into-high-quality-object-detection</href>

## 高品質な物体検知

二段階アーキテクチャ(領域候補の抽出と物体認識の二段階)を使うFaster-RCNNに着目した。

<img src='https://cdn-ak.f.st-hatena.com/images/fotolife/l/lib-arts/20191024/20191024215318.png'>


headとはRPNのこと。H0は画像をConvしたものからパッチごとに、そのパッチの中に物体の中心が含まれる信頼度、含まれるとしたらその物体のBB(大きさと位置)を出力するRPN。下図参照。(パッチをスライディングウィンドウと読み替えて)

<img src="https://drive.google.com/uc?export=view&id=1qFq6cmhp6Z-7KnRIOAszLl1IMJxS7dnA">

そうして各パッチに対して推定されたBBと正解のBBとのIoUを見て、採用するBBを決める。(そしてこのペアが次のH1における教師データになる？)

H1はBBの提案と画像をConvしたものを受け取る。そんでパッチごとに正解のBBであるgに近づけるようなLossとニューラルネット($f$のこと)を作る。
$$\mathcal{R}_{loc}[f]=\sum_iL_{loc}(f(\mathbf{x}_i,\mathbf{b}_i),\mathbf{g})$$
んで補正されたBBをB1として出力する。
H1はこれに加えてパッチごとにM+1クラス分類の確率を出力する。(Cとして。)

パッチごとにBBとクラスを推定したわけだが、こいつらと正解のBBとのIoUが閾値uを超えているパッチのBBのみ採用し、あとはクラスを「背景」とし、BBの推定は「negative」となる。そして、IoUが閾値を超えているパッチでの推定BB(b)と正解BB(g)をセットにした学習データを作る。

閾値が低いと各段階で豊富な訓練データを作ることができる。(つまりBBを補正するネットワークを学習しやすくなる。)　けどBBを推定するネットワークの学習ができない。逆にすると今度は訓練データがあんまりいいのが手に入らなくなる。（元々正解BBと近いBBしか採用しないで、これを訓練データとしてるんだから当然だよね）(これが高品質のパラドックス？)

#### なぜ先行研究では高品質化のアプローチが取られていないのか？

第一に、タスク(COCOとかKITTIとか)の評価がそのようになっていないことが挙げられる。

第二に、高品質オブジェクト検出器の設計は、高品質検出のパラドックスのため、既存のアプローチの自明な一般化ではないことだ。パラドックスを克服するためには、<strong>仮説生成器(多分BBを作るとこ)</strong>と<strong>物体検出器(BBの補正とClassifier)</strong>の品質を一致させる必要がある。文献では、例えば反復境界ボックス回帰[18]、[19]あるいはより良いRPN設計 [2] 、[63]によって仮説生成器の質を向上させる努力と、例えばIoUしきい値の集合[37]の積分損失を使用することによって物体検出器の質を向上させる努力があった。これらの試みは、両方のタスクの品質を同時に増加させる必要があるという事実を見落として、目標の一つだけを考慮するので、高品質検出を保証できない。まず、物体検出器が低品質のままであれば、仮説生成器の品質を上げることはほとんど利益がない。///なぜならば、物体検出器は低品質な仮説生成器からの出力を高品質に識別するように訓練されていないからである。一方、検出器の品質だけを向上させると、分類するための仮説生成器からの高品質な出力が少なすぎて、検出の改善が得られない。実際、図4(左)に示されているように、陽性サンプルの組はuとともに急速に減少するので、高u検出器はオーバーフィッティングしやすい。したがって、図2 (c) に示すように、高u検出器は、低u検出器よりもオーバーフィットしやすく、性能が悪い。

## Cascade R-CNN
カスケードR-CNNのアーキテクチャを図3 (b) に示す。
<img src='https://cdn-ak.f.st-hatena.com/images/fotolife/l/lib-arts/20191024/20191024215318.png'>
これは図3 (a) のFaser R-CNNアーキテクチャの多段拡張である。本研究では、提案検出に図3 (a) のRPN[47]を単純に採用し、検出サブネットワークに焦点を当てた。しかし、Cascade R-CNNはこの提案メカニズムに限定されるものではなく、他の選択肢も可能である。上のセクションで論じたように、目標は仮説生成器と物体検出器の質を同時に高め、高品質の物体検出を可能にすることである。これは、カスケードされた境界ボックス回帰とカスケードされた検出の組み合わせで達成される。

### Cascade Bounding Box Regression
質の高い仮説生成器は、正解のBBが利用可能であれば、訓練中に簡単に作り出すことができる。例えば、正解BBの周りをサンプリングすれば良い。大変なのは、正解BBが得られない場合に高品質なBBの推論を行うことである。この問題は、カスケードされたバウンディングボックス回帰を使用することで解決される。

$$f(\mathbf{x},\mathbf{b})=f_T\circ f_{T-1}\circ\cdots\circ f_1(\mathbf{x},\mathbf{b})$$

ここで、$T$はカスケードステージの総数である。重要なのは、各回帰関数$f_t$が、初期分布$\mathbf{b}_1$ではなく、前の回帰関数によって生成されたBB分布$\mathbf{b}_t$に対して最適化されることである。このようにして、仮説生成器は漸進的に改善される。

### Cascaded Detection

図4の左に示すように、RPNによる初期仮説分布は質の低い方向に大きく傾いている。
<img src='https://cdn-ak.f.st-hatena.com/images/fotolife/l/lib-arts/20191024/20191024231145.png'>
例えば、IoUしきい値u=0.7で陽性となる例はわずか2.9%であり、高品質の物体検出器を訓練することは困難である。Cascade R‐CNNは、再サンプリング機構としてカスケード回帰を使用する。
あるuに対して訓練されたBBの回帰関数(各パッチでBBを修正するNN)は、より高いIoUスコアを出すBBを生成する傾向があることから、カスケード回帰は、例えば{$\mathbf{x}_i,\mathbf{b}_i$}から始めて、より高いIoUの例分布{$\mathbf{x}'_i,\mathbf{b}'_i$}を連続的に再サンプリングすることにより、物体検出器の品質(閾値)uが増加しても、連続するステージの正の例のセットがほぼ一定のサイズを保つことができる。

各ステージ$t$で、headはClassifierである$h_t$とBB回帰関数$f_t$を内臓している。そしてこれら$h_t$,$f_t$はそのステージにおけるIoU閾値$u^t$に最適化している。